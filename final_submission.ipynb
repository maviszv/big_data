{"cells":[{"cell_type":"code","source":["from plotly.offline import plot\nfrom plotly.graph_objs import *\nimport numpy as np\nfrom graphframes import *\nimport numpy as np\nfrom pyspark.sql.functions import lit, col, create_map\nfrom itertools import chain"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/one_day_group.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"True\"\nfirst_row_is_header = \"True\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Create a view or table\n\ntemp_table_name = \"one_day_group_csv\"\n\ndf.createOrReplaceTempView(temp_table_name)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["#df contains 4million rows\ndf.show(10)\ndf2=df.toPandas()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def minute_partitioner(key):\n  \n    return hash(key)\n  \n# Validate results\n# num_partitions = 5\n# print(minute_partitioner(3) % num_partitions)\n# print(minute_partitioner(2) % num_partitions)\n# print(minute_partitioner(1) % num_partitions)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# ALL the entries \n#df_preprocess is the dataframe without key-value pair\ndf_all=df2.drop([\"_c0\"], axis=1)\ndf_create=spark.createDataFrame(df_all)\ndf_rename = df_create.withColumnRenamed(\"Time_in_minute\", \"Latencyminute\")\ndf_rename1=df_rename.withColumnRenamed(\"Time\",\"Latency_timestamp\")\ndf_preprocess=df_rename1.withColumn(\"Key\", df_rename [\"Latencyminute\"])\ndf_preprocess.show(10) "],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# creating key-value pair in the dataframes and prepare for hash partition based on key: PartitionBy()\nkey_dataframe = create_map(list(chain(*(\n    (lit(name), col(name)) for name in df_preprocess.columns if \"Latency\" in name\n)))).alias(\"Latency\")\n\ndf_final=df_preprocess.select(\"Key\", key_dataframe)\ndf_final.show(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Actual data parallelize stage based on key produced by binning the seconds in the dataset\n\nrdd_final =sc.parallelize(df_final.collect()) \\\n        .map(lambda df_final: (df_final['Key'], df_final)) \\\n        .partitionBy(12, minute_partitioner)\n\n    \nprint(\"Number of partitions: {}\".format(rdd_final.getNumPartitions()))\nprint(\"Partitioner: {}\".format(rdd_final.partitioner))\nprint(\"Partitions structure: {}\".format(rdd_final.glom().collect()))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["fractions =  { 12:0.05 }\napproxSample1 = rdd_final.sampleByKey(False, fractions)\nprint(approxSample1.take(100))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#return the parallelised data aboove into a dataframe\nrdd2 = approxSample1.flatMapValues(lambda x : [ (k, x[k]) for k in x.keys()])\nrdd2.map(lambda x : (x[0], x[1][0], x[1][1]))\\\n    .toDF((\"key\", \"Latency_timestamp\", \"Latency\"))\\\n    .show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#It is also possible to display it on HTML page, but it is not necessary here \np = plot(\n  [\n#     Histogram2dContour(x=x, y=y, contours=Contours(coloring='heatmap')),\n    Scatter(x=x, y=y, mode='markers')\n  ],\n  output_type='div'\n)\n\ndisplayHTML(p)"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"final_submission","notebookId":732515359512162},"nbformat":4,"nbformat_minor":0}
